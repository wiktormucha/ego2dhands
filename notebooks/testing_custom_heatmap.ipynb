{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''File to test the results of model in FreiHand dataset - equivelant to test_all_models.py but does it in jupyter'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/caa/Homes01/wmucha/venvs/hands_keypoints/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1554: UserWarning: Using default interpolation INTER_NEAREST, which is sub-optimal.Please specify interpolation mode for downscale and upscale explicitly.For additional information see this PR https://github.com/albumentations-team/albumentations/pull/584\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from datasets.FreiHAND import FreiHAND, FreiHAND_evaluation,FreiHAND_albu\n",
    "from models.models import CustomHeatmapsModel,EfficientWaterfall\n",
    "from utils.testing import batch_epe_calculation,batch_auc_calculation,batch_pck_calculation, show_batch_predictions\n",
    "from utils.utils import heatmaps_to_coordinates\n",
    "from config import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data_dir\": \"/data/wmucha/datasets/FreiHAND\",\n",
    "    \"model_path\": \"/caa/Homes01/wmucha/repos/applied_deep_learning/applied_dl/waterfall_fulldata_scratch9_121\",\n",
    "    \"test_batch_size\": 1,\n",
    "    \"device\": TESTING_DEVICE\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 13024\n"
     ]
    }
   ],
   "source": [
    "val_img_transform = val_image_transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(MODEL_IMG_SIZE),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=TRAIN_DATASET_MEANS, std=TRAIN_DATASET_STDS)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "# test_dataset = FreiHAND(config=config, set_type=\"test\", img_transform=val_img_transform)\n",
    "test_dataset = FreiHAND_albu(config=config, set_type=\"test\", albumetations = ALBUMENTATION_VAL)\n",
    "final_evaluation = FreiHAND_evaluation(config, img_transform= val_img_transform)\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    config[\"test_batch_size\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "final_evaluation_dataloader = DataLoader(\n",
    "    final_evaluation,\n",
    "    config[\"test_batch_size\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/caa/Homes01/wmucha/venvs/hands_keypoints/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# model = CustomHeatmapsModel(3, 21)\n",
    "model = EfficientWaterfall(N_KEYPOINTS)\n",
    "model.load_state_dict(\n",
    "    torch.load(config[\"model_path\"], map_location=torch.device(config[\"device\"]))\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, using_heatmaps = True, batch_size = 0):\n",
    "    accuracy_all = []\n",
    "    image_id = []\n",
    "    pred = []\n",
    "    gt = []\n",
    "    pck_acc = []\n",
    "    epe_lst = []\n",
    "    auc_lst = []\n",
    "   \n",
    "    for data in tqdm(dataloader):\n",
    "        inputs = data[\"image\"]\n",
    "        pred_heatmaps = model(inputs)\n",
    "        pred_heatmaps = pred_heatmaps.detach().numpy()\n",
    "        true_keypoints = (data[\"keypoints\"]).numpy()\n",
    "\n",
    "        # heatmaps = data[\"heatmaps\"]\n",
    "        # print(heatmaps.shape)\n",
    "        # heatmaps.resize(1,21,MODEL_IMG_SIZE,MODEL_IMG_SIZE)\n",
    "        # print(heatmaps.shape)\n",
    "        # true_keypoints = heatmaps_to_coordinates(np.array(heatmaps)) * MODEL_IMG_SIZE\n",
    "       \n",
    "        # keypoints = sample[\"keypoints\"]\n",
    "        # true_keypoints = keypoints[0] #* MODEL_IMG_SIZE\n",
    "\n",
    "        if using_heatmaps == True:\n",
    "            pred_keypoints = heatmaps_to_coordinates(pred_heatmaps)\n",
    "        else:\n",
    "            pred_keypoints = pred_heatmaps.reshape(batch_size,N_KEYPOINTS,2)\n",
    "\n",
    "        # print(true_keypoints)\n",
    "        # print(pred_keypoints)\n",
    "        accuracy_keypoint = ((true_keypoints - pred_keypoints) ** 2).sum(axis=2) ** (1 / 2)\n",
    "        accuracy_image = accuracy_keypoint.mean(axis=1)\n",
    "        accuracy_all.extend(list(accuracy_image))\n",
    "\n",
    "        # Calculate PCK@02\n",
    "        avg_acc = batch_pck_calculation(pred_keypoints, true_keypoints, treshold = 0.2, mask = None, normalize = None)\n",
    "        pck_acc.append(avg_acc)\n",
    "\n",
    "        # Calculate EPE mean and median, mind that it depends on what scale of input keypoints \n",
    "        epe = batch_epe_calculation(pred_keypoints, true_keypoints)\n",
    "        epe_lst.append(epe)\n",
    "\n",
    "        # AUC calculation\n",
    "        auc = batch_auc_calculation(pred_keypoints, true_keypoints, num_step=20, mask = None)\n",
    "        auc_lst.append(auc)\n",
    "        # break\n",
    "        if avg_acc < 0.5:\n",
    "            show_batch_predictions(data, model, epe)\n",
    "\n",
    "    pck = sum(pck_acc) / len(pck_acc)\n",
    "    epe_final = sum(epe_lst) / len(epe_lst)\n",
    "    auc_final = sum(auc_lst) / len(auc_lst)\n",
    "\n",
    "    lines = [pck,epe_final,auc_final]\n",
    "    with open('results.txt', 'w') as f:\n",
    "        f.writelines(lines)\n",
    "    print (f'PCK@2: {pck}, EPE: {epe_final}, AUC: {auc_final}')\n",
    "    return accuracy_all, pck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_all, pck = evaluate(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.mean(accuracy_all)\n",
    "print(\"Average error per keypoint: {:.1f}% from image size\".format(error * 100))\n",
    "\n",
    "for img_size in [RAW_IMG_SIZE, MODEL_IMG_SIZE]:\n",
    "    error_pixels = error * img_size\n",
    "    image_size = f\"{img_size}x{img_size}\"\n",
    "    print(\n",
    "        \"Average error per keypoint: {:.0f} pixels for image {}\".format(\n",
    "            error_pixels, image_size\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_dataloader:\n",
    "    show_batch_predictions(data, model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on final evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_all, pck = evaluate(model,final_evaluation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.mean(accuracy_all)\n",
    "print(\"Average error per keypoint: {:.1f}% from image size\".format(error * 100))\n",
    "\n",
    "for img_size in [RAW_IMG_SIZE, MODEL_IMG_SIZE]:\n",
    "# for img_size in [1, RAW_IMG_SIZE/MODEL_IMG_SIZE]:\n",
    "    error_pixels = error * img_size\n",
    "    image_size = f\"{img_size}x{img_size}\"\n",
    "    print(\n",
    "        \"Average error per keypoint: {:.0f} pixels for image {}\".format(\n",
    "            error_pixels, image_size\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in final_evaluation_dataloader:\n",
    "    show_batch_predictions(data, model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m65"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('hands_keypoints')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a280fb21c1423496fb34162d68394b6a193e709eb8d743bf104afa5ed9d41e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
